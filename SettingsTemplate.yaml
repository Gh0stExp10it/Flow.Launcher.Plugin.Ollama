body:
  - type: input
    attributes:
      name: ollama_host
      label: "Ollama Host:"
      defaultValue: "http://localhost:11434"
      description: "URL of the local Ollama instance to communicate via API. Default - http://localhost:11434"
  - type: input
    attributes:
      name: ollama_model
      label: "Ollama Model:"
      defaultValue: "tinyllama:1.1b"
      description: "The LLM to be used (model library https://ollama.com/library). Default - tinyllama:1.1b"
  - type: checkbox
    attributes:
      name: pull_model
      label: "Automatic Model Download"
      description: "Download LLM automatically if not already installed?"
      defaultValue: 'false'
  - type: checkbox
    attributes:
      name: save_response
      label: "Save Chat to File"
      description: "Should the chat be saved as a text file? This allows it to be opened directly in a text editor."
      defaultValue: 'true'
  - type: input
    attributes:
      name: prompt_stop
      label: "Prompt Stop:"
      defaultValue: "||"
      description: "Characters to indicate end of prompt. Default - ||"
  - type: dropdown
    attributes:
      name: log_level
      label: "Log Level:"
      defaultValue: ERROR
      options:
        - DEBUG
        - INFO
        - WARNING
        - ERROR
        - CRITICAL
